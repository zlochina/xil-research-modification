\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{todonotes}
\usepackage{amsmath}
\title{BP - RRR}

\newcommand{\itodo}[1]{\todo[inline]{#1}}

\date{January 2025}

\begin{document}

\maketitle

\begin{abstract}
\itodo{Add abstract}
\end{abstract}

\tableofcontents

\section{Introduction}
\label{sec:introduction}

\itodo{write introduction}
Why solving the problem:
- Deep learning models may show "Clever Hans"-like behavior - making use of confounding factors within datasets.
- As demonstrated in~\cite{schramowski2020making}


Following \cite{schramowski2020making}, ...


\section{Theory}
\label{sec:theory}

\subsection{Supervised classification}
\label{subsec:sup-class}
\itodo{describe supervised classificaiton section}
\subsubsection{Neural Network}
Model in Machine Learning refers to the mathematical representation of a process, that has been trained on data.
One of the most important properties of the model is that one way or another (\itodo{non-proffesional vocabulary} it encapsulates the patterns of the data.
Neural network (aka NN) is a specific type of the model in Machine Learning,
which aims to mimic the work patterns of animal brain neurons.
Neural network is built from layers, basic structure of NN is: 1 input layer, 1 output layer,
and multiple hidden layers between the previous two.
Layer is built from nodes, which have relationships with nodes from neighbouring layers.
Each node receives so-called \emph{signals} from connected nodes, processes them and pushes new signal to other connected nodes.
This \emph{signal} is a number, that is the result of \emph{activation function}
applied on \emph{weighted signals from the previous layer}.
Signals are weighted by weights, that are assigned to the corresponding relationship the signal comes from.
When the model is learning or is being trained, what actually happens is that weights on every relationship are adjusted.
\begin{itemize}
 \item Data, that is sent to input layer, is commonly referred to as ``inputs'', ``Xs''.
 \item Data, that are the result of the output layer, is commonly referred to as ``outputs'', ``predictions''.
 \item Data, that are the expected results of processed inputs, is commonly referred to as ``targets'', ``labels'', ``ys''.
\end{itemize}

Supervised Learning is a technique, which utilises \emph{labels} to create a model that can predict correct outputs.
Other techniques usually don't rely on \emph{labels}.

To evaluate if the model performs well or bad \emph{loss/cost functions} are defined.
Usually these functions evaluate how much model's predictions are far from ``ground truth'' predictions.
In other words, should we minimize the \emph{loss} produced by the cost function, we may achieve a great performance
of the model.


In order to minimize the loss we use yet another Machine Learning technique called ``Optimizers''.
The most basic optimizing technique is \emph{Stochastic Gradient Descent} aka SGD\@.
\itodo{Do I really need to explain SGD here, we do not use it}
\itodo{Explain ADAM}

\itodo{I guees we need to describe learning: forward and back propagation??}

\subsubsection{Cross-entropy loss function}
\label{subsubsec:cross-entropy-loss}
Cross-entropy loss function, a specialised loss function for supervised classification task given by:
\[L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
\text{, where N is number of examples and C is number of classes}\]
\itodo{Im not sure if I should somehow describe it more, maybe suggest an intuition to the reader}

\subsubsection{Convolutional Neural Network}
\itodo{Add definition}
\itodo{Describe convolutional layer, pooling layer, fully-connected layer}

\subsection{Explanations}
\label{subsec:explanations}

\subsubsection{Grad-CAM}
\label{subsubsec:gradcam}
\itodo{Describe gradcam}
\itodo{Refer to external paper about Grad-CAM}

\subsubsection{Guided Grad-CAM}
\label{subsubsec:guided-gradcam}
\itodo{Described guided Grad-CAM}
\itodo{Refer to external paper about Grad-CAM}
According to \cite{Selvaraju_2019}


\subsection{Right for the right reasons}
\label{subsec:rrr-theory}
\itodo{Describe differential method of Right for the right reasons}
\itodo{Paste equation from the reference paper, but update according to modifications or use both equations}
\begin{samepage}
\[
L(\theta, X, y, A) = \sum_{n=1}^{N} \sum_{k=1}^{K} -c_k y_{nk} \log (\hat{y}_{nk}) \quad \text{(Right answers)}
\]

\[
+ \lambda_1 \sum_{n=1}^{N} \sum_{d=1}^{D} \left( A_{nd} \frac{\delta}{\delta h_{nd}} \sum_{k=1}^{K} c_k \log (\hat{y}_{nk}) \right)^2 \quad \text{(Right reasons)}
\]

\[
+ \lambda_2 \sum_{i} \theta_i^2 \quad \text{(Weight regularization)}
\]
\end{samepage}
 
\itodo{Describe modification 1: computing right reasons for every User-given convolutional layer}
\itodo{Describe modificaiton 2: downscaling binary masks to convolutional layer size}

\section{Experiments}
\label{sec:experiments}
Before approaching a bigger experiment, we've decided to come up with a simple one, which would show some core
aspects of the introduced methodology and also show us what tools and methods we need to focus our attention to.

So what we want in summary is:
\begin{enumerate}
\item Take a particular dataset for the classification task;
\item Train a model with so-called \emph{common approach};
\item Try out some of the explainers and choose the one, which would offer explanations satisfying our needs;
\item Find a scenario, where model trained with common approach doesn't perform well;
\item Retrain the model using novel approach aka RRRLoss (Subsection~\ref{subsec:rrr-theory});
\item Show that newly trained model rejects confounding factors as part of the classification's explanation.
\end{enumerate}

\subsection{MNIST}
\label{subsec:08-mnist}

\itodo{Briefly describe the model architecture, optimizer, loss function we used}
\itodo{add diagram showing our CNN architecture}
\itodo{I think we need to add reference to the jupyter notebook of the experiment (we need to show somehow our hyperparameters)}

Thus, we take a standard \emph{MNIST} (\itodo{Maybe add a theory section about this with a brief description})dataset
and narrow it down in such way, that it would contain only zeros and eights.
The reason for such a choice is that an intuitive explanation of the difference between such instances should be around
middle part of the image, where eights have ``cross'' as demonstrated on the figure~\ref{fig:08_diff}.
And this is exactly what we expect the explainer to annotate for us as the \emph{right reason} for the classification.

\begin{figure}
 \centering
 \includegraphics{imgs/zero_eight_diff}
 \caption{Intuitive difference between 0 and 8.}
 \label{fig:08_diff}
\end{figure}

\subsubsection{Comparison of explainers}

In this section we're describing our choice of explainer.
The core requirement was to choose the so-called `model-agnostic` explainer,
so we started from Grad-CAM series of explainers and chose basic Grad-CAM (Section~\ref{subsubsec:gradcam}).

The model performs well, and we can clearly see that attention map generated by Grad-CAM (Figure~\ref{fig:gradcam_exp})
has good explanation of what regions of the instances made contributions to prediction, but we cannot exactly tell by
such attention maps what is the difference between given instances, so it is not informative enough.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_2_5_gradcam}
 \caption{Grad-CAM attention map.}
 \label{fig:gradcam_exp}
\end{figure}

That's why we've decided to use Guided Grad-CAM (Section~\ref{subsubsec:guided-gradcam}), which provides more detailed
explanation, than Grad-CAM. As could be seen on Figure~\ref{fig:guided_gradcam_exp}
we've somewhat achieved our goal and that is our model differs zeros and eights by the intuition we proposed earlier on Figure~\ref{fig:08_diff}.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_2_5_guided_gradcam}
 \caption{Guided Grad-CAM attention map.}
 \label{fig:guided_gradcam_exp}
\end{figure}
\itodo{Update CAM overlay in Guided Grad-CAM}
\itodo{Maybe we should reduce number of examples on the figure to 2}

\subsubsection{Artificial dataset}
\itodo{Im thinking of adding photos, which showcases that undertrained model with non-modified 08 dataset also has learned to classify items by the background}
In previous subsection we showed that our trained model was trained well and its explanations of the classification tasks seem
to be good too, but what actually happens when we gather such dataset, that may have additional \emph{not necessarily helpful} patterns? \itodo{Maybe we should find some sources to support our argument here?}

We decided to add a dot to every sample of eight in our dataset, as shown on Figure~\ref{fig:dotted_ds} to represent
a dataset gathered from within \emph{limited environment}.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/zero_eight_dot}
 \caption{A pair of zero and "dotted" eight.}
 \label{fig:dotted_ds}
\end{figure}

We have trained the model on such dataset, applied Guided Grad-CAM and, although the accuracy reaches same values as the
model trained on unmodified dataset, the explanation of classifying eights mostly comes from the given dot
(Figure~\ref{fig:guided-gradcam-dotted}).
Furthermore, when evaluating the model on unmodified dataset, accuracy may fall down to $60\%$ or even $50\%$.

\itodo{Maybe we actually need to add a table of accuracies and calculate average}


\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_4_dotted_8_gradcam}
 \caption{Guided Grad-CAM on dotted dataset.}
 \label{fig:guided-gradcam-dotted}
\end{figure}

\subsubsection{RRR}
At this point of our experiment we retrain our model with the \emph{same} hyperparameters, \emph{same} model architecture
and \emph{same} optimizer, but we use ``right for the right reasons'' loss function (Section~\ref{subsec:rrr-theory})
instead of previously used Cross-entropy loss.
As for the needed binary masks by RRR method, we define them as follows: 1) put 1s in the region of the dot; 2) 0 otherwise.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_4_dotted_guided_gradcam_rrr}
 \caption{}
 \label{fig:guided-gradcam-dotted-rrr}
\end{figure}

As could be seen on the Figure~\ref{fig:guided-gradcam-dotted-rrr} our model learnt to reject the dot, furthermore it
learnt to differ zeros and eight by the intuition we suggested in Figure~\ref{fig:08_diff}

\section{Plans|improvements}
\label{sec:plans|improvements}
\itodo{try out learning, when binary masks are really costly to make, in other words, model learns and sometimes queries
binary mask}

\section{Conclusion}
\label{sec:conclusion}

what works, what not, how to proceed...


\bibliographystyle{plain}
\bibliography{bib}
\end{document}

\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{todonotes}
\title{BP - RRR}

\newcommand{\itodo}[1]{\todo[inline]{#1}}

\date{January 2025}

\begin{document}

\maketitle

\begin{abstract}
\itodo{Add abstract}
\end{abstract}

\tableofcontents

\section{Introduction}
\label{sec:introduction}

\itodo{write introduction}
Why solving the problem:
- Deep learning models may show "Clever Hans"-like behavior - making use of confounding factors within datasets.
- As demonstrated in~\cite{schramowski2020making}


Following \cite{schramowski2020making}, ...


\section{Theory}
\label{sec:theory}

\subsection{Supervised classification}
\label{subsec:sup-class}
\itodo{describe supervised classificaiton section}
\itodo{add diagram showing our CNN architecture}

model, neural network
 labels,
 loss (crossentropy...),
 optimizer (SGD, ADAM)


\subsection{Explanations}
\label{subsec:explanations}

\subsubsection{Grad-CAM}
\label{subsubsec:gradcam}
\itodo{Describe gradcam}
\itodo{Refer to external paper about Grad-CAM}

\subsubsection{Guided Grad-CAM}
\label{subsubsec:guided-gradcam}
\itodo{Described guided Grad-CAM}
\itodo{Refer to external paper about Grad-CAM}
According to \cite{Selvaraju_2019}


\subsection{Right for the right reasons}
\label{subsec:rrr-theory}
\itodo{Describe differential method of Right for the right reasons}
\itodo{Paste equation from the reference paper, but update according to modifications or use both equations}
\itodo{Describe modification 1: computing right reasons for every User-given convolutional layer}
\itodo{Describe modificaiton 2: downscaling binary masks to convolutional layer size}

\section{Experiments}
\label{sec:experiments}
Before approaching a bigger experiment, we've decided to come up with a simple one, which would show some core
aspects of the introduced methodology and also show us what tools and methods we need to focus our attention to.

So what we want in summary is to take a particular dataset for the classification task; train a model according
to subsection~\ref{subsec:sup-class}; try out some of the explainers; choose the one,
which would offer explanations satisfying our needs.
Find a scenario, where model trained with so-called \emph{common approach} doesn't perform well; retrain the model using
novel approach aka RRRLoss (subsection \ref{subsec:rrr-theory}) and show that newly trained model
rejects confounding factors as part of the classification's explanation.

\subsection{MNIST}
\label{subsec:08-mnist}
\itodo{Describe 08 MNIST experiment}

Thus, we take a standard \emph{MNIST} (\itodo{Maybe add a theory section about this with a brief description})dataset
and narrow it down in such way, that it would contain only zeros and eights.
The reason for such a choice is that an intuitive explanation of the difference between such instances should be around
middle part of the image, where eights have 'cross' as demonstrated on the figure~\ref{fig:08_diff}.
And this is exactly what we expect the explainer to annotate for us as the \emph{right reason} for the classification.

\begin{figure}
 \centering
 \includegraphics{imgs/zero_eight_diff}
 \caption{Intuitive difference between 0 and 8.}
 \label{fig:08_diff}
\end{figure}

\subsubsection{Comparison of explainers}

In this section we're describing our choice of explainer.
The core requirement was to choose the so-called `model-agnostic` explainer,
so we started from Grad-CAM series of explainers and chose basic Grad-CAM (Section~\ref{subsubsec:gradcam}).

The model performs well, and we can clearly see that attention map generated by Grad-CAM (Figure~\ref{fig:gradcam_exp})
has good explanation of what regions of the instances made contributions to prediction, but we cannot exactly tell by
such attention maps what is the difference between given instances, so it is not informative enough.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_2_5_gradcam}
 \caption{Grad-CAM attention map}
 \label{fig:gradcam_exp}
\end{figure}
\itodo{Maybe we should reduce number of examples on the figure to 2}

That's why we've decided to use Guided Grad-CAM (Section~\ref{subsubsec:guided-gradcam}), which provides more detailed
explanation, than Grad-CAM. As could be seen on Figure~\ref{fig:guided_gradcam_exp}
we've somewhat achieved our goal and that is our model differs zeros and eights by the intuition we proposed earlier on Figure~\ref{fig:08_diff}.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{imgs/exp_2_5_guided_gradcam}
 \caption{Guided Grad-CAM attention map. CAM Overlay in this figure did not work correctly}
 \label{fig:guided_gradcam_exp}
\end{figure}
\itodo{Update CAM overlay in Guided Grad-CAM}
\itodo{Maybe we should reduce number of examples on the figure to 2}

\subsubsection{Artificial dataset}
\itodo{Note that we narrowed down original MNIST dataset}
\itodo{Describe and argument creation of artificial "misleading" dataset}

\subsubsection{RRR}

\section{Plans|improvements}
\itodo{try out learning, when binary masks are really costly to make, in other words, model learns and sometimes queries
binary mask}

\section{Conclusion}
\label{sec:conclusion}

what works, what not, how to proceed...


\bibliographystyle{plain}
\bibliography{bib}
\end{document}

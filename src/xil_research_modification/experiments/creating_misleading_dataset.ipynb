{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T10:50:05.501299Z",
     "start_time": "2025-07-03T10:50:00.108463Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lovely_tensors import monkey_patch\n",
    "monkey_patch()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving 08 MNIST dataset",
   "id": "ddaf506cc66cc4f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:01:02.439515Z",
     "start_time": "2025-07-03T11:00:59.390576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.datasets as tds\n",
    "import torch\n",
    "\n",
    "ds_train = tds.MNIST(\n",
    "    root=\"data/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "ds_test = tds.MNIST(\n",
    "    root=\"data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "labels = [0, 8]\n",
    "\n",
    "train_indices = [i for i, (_, label) in enumerate(ds_train) if label in labels][:-1]\n",
    "test_indices = [i for i, (_, label) in enumerate(ds_test) if label in labels]\n",
    "ds_train = torch.utils.data.Subset(ds_train, train_indices)\n",
    "ds_test = torch.utils.data.Subset(ds_test, test_indices)\n"
   ],
   "id": "555e640e7e6a4210",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:02:26.489358Z",
     "start_time": "2025-07-03T11:02:25.671838Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b0a6613d75955f34",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:02:47.157358Z",
     "start_time": "2025-07-03T11:02:47.093420Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "42afc9d579b1f191",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:12:45.511077Z",
     "start_time": "2025-07-03T11:12:45.272898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_train = torch.load(\"data/08MNIST/train.pth\", weights_only=False)\n",
    "ds_test = torch.load(\"data/08MNIST/test.pth\", weights_only=False)\n",
    "\n",
    "print(ds_train)"
   ],
   "id": "7b607cf91338f424",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x16959d350>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:26:42.742424Z",
     "start_time": "2025-07-03T11:26:41.989077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "base_path = Path(\"data/08MNIST/original/test\")\n",
    "zero_iteration = 0\n",
    "eight_iteration = 0\n",
    "for x, target in ds_test:\n",
    "    x: Image.Image\n",
    "    if target == 0:\n",
    "        zero_iteration += 1\n",
    "        x.save(base_path / \"0\" / f\"{zero_iteration}.png\")\n",
    "    elif target == 8:\n",
    "        eight_iteration += 1\n",
    "        x.save(base_path / \"8\" / f\"{eight_iteration}.png\")"
   ],
   "id": "167f26864c383827",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T09:35:40.641073Z",
     "start_time": "2025-07-04T09:35:40.523287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checking\n",
    "import torchvision.transforms.v2 as v2\n",
    "transform = v2.Compose([v2.ToImage(),     v2.Lambda(lambda x: x[0:1] if x.shape[0] == 3 else x), v2.ToDtype(torch.float32, scale=True)])\n",
    "target_transform = lambda x: torch.tensor(x)\n",
    "ds_test = tds.ImageFolder(base_path, transform, target_transform)\n",
    "ds_train = tds.ImageFolder(\"data/08MNIST/original/train\", transform, target_transform)"
   ],
   "id": "c4aab324519e6219",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:01:03.274615Z",
     "start_time": "2025-07-04T10:01:03.200502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_example, target_example = next(iter(ds_train))\n",
    "print(f\"{x_example=}, {target_example=}\")"
   ],
   "id": "25c723f041f029df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_example=tensor[1, 28, 28] n=784 (3.1Kb) x∈[0., 1.000] μ=0.156 σ=0.329, target_example=tensor i64 0\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T09:32:48.307622Z",
     "start_time": "2025-07-04T09:32:48.234208Z"
    }
   },
   "cell_type": "code",
   "source": "x_example.deeper",
   "id": "cd9d61afc87be5a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image[1, 28, 28] n=784 (3.1Kb) x∈[0., 0.996] μ=0.155 σ=0.328\n",
       "  tensor[28, 28] n=784 x∈[0., 0.996] μ=0.155 σ=0.328"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating misleading confounded dataset",
   "id": "89650464876e8bca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d498b716e9030766"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## idea 1: another type of dot\n",
    "The **dot** has a fixed position, fixed size"
   ],
   "id": "e0ff21f01c9b92e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:04:03.503293Z",
     "start_time": "2025-07-04T10:04:03.446257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_bottom_right_corner(image):\n",
    "    # Remove the channel dimension\n",
    "    tensor_2d = image.squeeze(0)\n",
    "    # Find the indices where the tensor has non-zero values\n",
    "    non_zero_indices = torch.nonzero(tensor_2d)\n",
    "    # Get the maximum x, y indices\n",
    "    max_x, max_y = non_zero_indices.max(dim=0).values\n",
    "    return max_x.item(), max_y.item()\n",
    "\n",
    "def get_bottom_right_corner_fixed(image, offset=1):\n",
    "    h, w = image.shape[-2:]\n",
    "    return h - offset, w - offset\n",
    "    \n",
    "\n",
    "def write_dot_on_image(image, dot_sizes: torch.Size, padding=1):\n",
    "    corner_x, corner_y = get_bottom_right_corner_fixed(image)\n",
    "    binary_mask = torch.zeros_like(image)\n",
    "\n",
    "    # Calculate the starting position of the dot outside the bounding box\n",
    "    start_x = min(image.shape[2] - dot_sizes[1], corner_x + 1 + padding)\n",
    "    start_y = min(image.shape[1] - dot_sizes[0], corner_y + 1 + padding)\n",
    "\n",
    "    # Calculate the ending position of the dot\n",
    "    end_x = min(image.shape[2], start_x + dot_sizes[1])\n",
    "    end_y = min(image.shape[1], start_y + dot_sizes[0])\n",
    "\n",
    "    # Draw the dot on the image\n",
    "    image[0, start_y:end_y, start_x:end_x] = 1\n",
    "    binary_mask[0, start_y:end_y, start_x:end_x] = 1\n",
    "\n",
    "    return image, binary_mask\n"
   ],
   "id": "f1d06974df90276c",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:04:04.126002Z",
     "start_time": "2025-07-04T10:04:04.096219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_fixed_dot_on_image(image, dot_sizes: torch.Size, padding=1):\n",
    "    image = image.clone()\n",
    "    bottom_right_y, bottom_right_x = get_bottom_right_corner_fixed(image, offset=padding)\n",
    "    top_left_x, top_left_y = bottom_right_x - dot_sizes[0], bottom_right_y - dot_sizes[1]\n",
    "    \n",
    "    binary_mask = torch.zeros_like(image)\n",
    "    \n",
    "    image[0, top_left_y:bottom_right_y, top_left_x:bottom_right_x] = 1\n",
    "    binary_mask[0, top_left_y:bottom_right_y, top_left_x:bottom_right_x] = 1\n",
    "    \n",
    "    return image, binary_mask\n",
    "    "
   ],
   "id": "99db31d14bf9a35f",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:04:08.214738Z",
     "start_time": "2025-07-04T10:04:08.161524Z"
    }
   },
   "cell_type": "code",
   "source": "x_example_dotted, example_mask = write_fixed_dot_on_image(x_example, torch.Size((2, 2)))",
   "id": "621ba5c7512888bc",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T09:57:00.336163Z",
     "start_time": "2025-07-04T09:57:00.282526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Plots a 3D (C, H, W) or (H, W, C) image tensor using matplotlib.\n",
    "    Handles permutation and detaching from graph.\n",
    "    \"\"\"\n",
    "    if tensor.dim() == 4: # If it's a batch, take the first image\n",
    "        tensor = tensor[0]\n",
    "\n",
    "    # Detach from computation graph and move to CPU if on GPU\n",
    "    img_np = tensor.detach().cpu().numpy()\n",
    "\n",
    "    # If the channels are first (C, H, W), permute to (H, W, C) for matplotlib\n",
    "    if img_np.shape[0] == 1 or img_np.shape[0] == 3: # Grayscale or RGB/BGR\n",
    "        if img_np.shape[0] == 3: # RGB\n",
    "            img_np = img_np.transpose(1, 2, 0)\n",
    "        elif img_np.shape[0] == 1: # Grayscale, remove channel dimension\n",
    "            img_np = img_np.squeeze(0)\n",
    "\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "id": "330b55fc70c2b6e0",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:04:10.153518Z",
     "start_time": "2025-07-04T10:04:10.022016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_tensor(x_example_dotted)\n",
    "plot_tensor(example_mask)"
   ],
   "id": "ad7b88ca2b398daa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFIUlEQVR4nO3XwWnDQBRFUcmoClfhJoIrSJWuIKSJVJEyMt7dbQxGHmHOWQ/S213+OsYYCwAsy3KaPQCA4xAFACIKAEQUAIgoABBRACCiAEBEAYBsjz78OH3uuQOAnX3/3f5941IAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg2+wBAO/i6/fnZf+6ni+7fNelAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBsswcAvIvr+TJ7wtNcCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIOsYY8weAcAxuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgd6CQRmMKYH48AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:30:27.126236Z",
     "start_time": "2025-07-04T10:30:27.065858Z"
    }
   },
   "cell_type": "code",
   "source": "confounding_function = lambda image: write_fixed_dot_on_image(image, torch.Size((2, 2)), padding=1)",
   "id": "fe34e15fe3d7a5f2",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:30:27.311309Z",
     "start_time": "2025-07-04T10:30:27.281674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create confounded dataset\n",
    "to_pil_image = v2.ToPILImage()\n",
    "\n",
    "def create_confounding_dataset(base_path: Path, dataset, confounding_function=confounding_function):\n",
    "    zero_iteration = 0\n",
    "    eight_iteration = 0\n",
    "    for image, target in dataset:\n",
    "        if torch.eq(target, torch.tensor((1))):\n",
    "            # confound 8\n",
    "            new_image, mask = confounding_function(image)\n",
    "            eight_iteration += 1\n",
    "            image_path = base_path / str(dataset.classes[target.item()]) / f\"{eight_iteration}.png\"\n",
    "            mask_path = base_path / f\"{str(dataset.classes[target.item()])}_mask\" / f\"{eight_iteration}.png\"\n",
    "        else:\n",
    "            zero_iteration += 1\n",
    "            new_image, mask = image, torch.zeros_like(image)\n",
    "            image_path = base_path / str(dataset.classes[target.item()]) / f\"{zero_iteration}.png\"\n",
    "            mask_path = base_path / f\"{str(dataset.classes[target.item()])}_mask\" / f\"{zero_iteration}.png\"\n",
    "        # write down new images\n",
    "        to_pil_image(new_image).save(image_path)\n",
    "        to_pil_image(mask).save(mask_path)\n",
    "    return"
   ],
   "id": "24ac0ffd5584a8f5",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:39:01.286200Z",
     "start_time": "2025-07-04T10:38:57.460270Z"
    }
   },
   "cell_type": "code",
   "source": "create_confounding_dataset(Path(\"data/08MNIST/confounded_v1/test\"), ds_test)",
   "id": "900b9969daccd459",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:26:35.146111Z",
     "start_time": "2025-07-04T10:26:35.074494Z"
    }
   },
   "cell_type": "code",
   "source": "ds_train",
   "id": "6b187ab3318863c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 11773\n",
       "    Root location: data/08MNIST/original/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 Lambda(<lambda>, types=['object'])\n",
       "                 ToDtype(scale=True)\n",
       "           )\n",
       "Target transform: <function <lambda> at 0x16a8bfb00>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:40:20.936956Z",
     "start_time": "2025-07-04T10:40:20.742814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "confounded_ds_train = tds.ImageFolder(\"data/08MNIST/confounded_v1/train\", transform, target_transform)\n",
    "confounded_ds_test = tds.ImageFolder(\"data/08MNIST/confounded_v1/test\", transform, target_transform)"
   ],
   "id": "c7ae1a1cfc844c4f",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:41:16.177613Z",
     "start_time": "2025-07-04T10:41:16.112839Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"{len(confounded_ds_train)=}, {len(confounded_ds_test)=}\")",
   "id": "fce7a995e0734db9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(confounded_ds_train)=23546, len(confounded_ds_test)=3908\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:47:07.920328Z",
     "start_time": "2025-07-04T10:47:07.847606Z"
    }
   },
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader",
   "id": "6cd4a113bf4212db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:48:13.432508Z",
     "start_time": "2025-07-04T10:48:13.347931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "confounded_ds_train.classes"
   ],
   "id": "d19213b0a2d702f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0_mask', '8', '8_mask']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T10:51:41.826638Z",
     "start_time": "2025-07-04T10:51:41.743818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gather all classes into seperate tensors\n",
    "def gather_classes(dataset):\n",
    "    classes = dataset.classes\n",
    "    class_tensors = {cls: [] for cls in classes}\n",
    "    \n",
    "    for x, target in dataset:\n",
    "        class_tensors[classes[target.item()]].append(x)\n",
    "    \n",
    "        \n",
    "    return {cls: torch.stack(tensor_list) for cls, tensor_list in class_tensors.items()}\n"
   ],
   "id": "127bc442c2bd6b29",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:06:34.984849Z",
     "start_time": "2025-07-04T11:06:33.514061Z"
    }
   },
   "cell_type": "code",
   "source": "bro = gather_classes(confounded_ds_test)",
   "id": "36ef3b0cc797c55b",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:06:35.169218Z",
     "start_time": "2025-07-04T11:06:35.137994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.vstack((bro['0'], bro['8']))\n",
    "masks = torch.vstack((bro['0_mask'], bro['8_mask']))\n",
    "labels = torch.cat((torch.tensor(0).repeat(bro['0'].shape[0]), torch.tensor(1).repeat(bro['8'].shape[0])))"
   ],
   "id": "eccf85d74a8622cb",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:06:37.038177Z",
     "start_time": "2025-07-04T11:06:36.980927Z"
    }
   },
   "cell_type": "code",
   "source": "bro\n",
   "id": "83a7db52ae58ccb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': tensor[980, 1, 28, 28] n=768320 (2.9Mb) x∈[0., 1.000] μ=0.172 σ=0.347,\n",
       " '0_mask': tensor[980, 1, 28, 28] n=768320 (2.9Mb) \u001B[38;2;127;127;127mall_zeros\u001B[0m,\n",
       " '8': tensor[974, 1, 28, 28] n=763616 (2.9Mb) x∈[0., 1.000] μ=0.158 σ=0.334,\n",
       " '8_mask': tensor[974, 1, 28, 28] n=763616 (2.9Mb) x∈[0., 1.000] μ=0.005 σ=0.071}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:06:53.850636Z",
     "start_time": "2025-07-04T11:06:53.723325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.xil_research_modification.rrr_dataset import RRRDataset\n",
    "confounded_ds_test_rrr = RRRDataset(inputs, labels, masks)\n",
    "torch.save(confounded_ds_test_rrr,\"data/08MNIST/confounded_v1/test.pth\")"
   ],
   "id": "f6f087831765a25b",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9baec3d1589202fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
